
# 최종 프로젝트 & MLOps

## 20211207
- 오늘 한것
    - MLOps 강의듣기
    - 멘토링
    - 최종 프로젝트 아이디어 회의
    - 면접 질문 정리

- 오늘 정리한 것
    - https://zeuskim.notion.site/MLOps-d06d1f9a67534ffdbad20c2fac7132fe

- 오늘 배운 것
    - MLOps 강의 1강
        - 전체 강의의 흐름 느끼기.
        - 꽃 피는 시기가 꽃마다 다른것처럼 꽃이 되기 위한 준비 과정일 뿐이야.
    - MLOps 강의 2강
        - MLOps는 춘추 전국 시대
        - 예상 트래픽, 서버의 CPU, Memory 성능, 스켈업 가능여부, 비용 등을 모두 고려해야함.
        - 인프라 구축
        - 배포
        - 모델 및 실험 관리 자동화
        - Feature 관리
        - 실제 데이터에 따른 모델 평가
        - 지속적으로 모델 업데이트
        - 모니터링
        - autoML같은 도구로 모델 생성까지 자동화
    - MLOps 강의 3강
        - 머신 러닝 서버란? 
            - 데이터 전처리, 모델 기반 예측 결과를 request가 오면 response로 반환해주는것
        - Online serving의 3가지 방법(실시간)
            - 직접 서버 개발해서 서빙 flask , django 처럼
            - 클라우드 이용
            - serving 라이브러리 이용
            - 순서는 클라우드 -> 직접개발 -> serving 순으로 해보면 좋다.
        - Batch serving 
            - 일정 주기마다 input을 batch로 모아서 inference하고 서빙

    - greedy decoding과 Beam search Decoding의 차이
        - greedy 디코딩은 단어를 생성할때 가장 확률이 높은 후보 하나만을 선택하는 방식
        - beam search decoding은 한 시점에서 예측된 단어들 중 확률이 가장 높은 K개를 선정해서 EOS가 K개가 나올때까지 반복하는 방식
    - 확률 모형과 확률 변수
        - 확률 변수는 사건을 확률로 표현하기 위해 사건을 정의 하는것
        - 확률 모형은 확률적 현상을 수학적으로 표현한것
    - TF-IDF는 어떤 단어가 한 문서에서 얼마나 자주 등장하는지(TF) 전체 문서에서 얼마나 자주 등장하지 않는지(IDF)를 수치화 한것
        - 단어를 숫자로 표현함
    - one-hot -> TF-IDF -> word2vec/Glove -> Contextualised Word Embedding
        - 위의 순서에서 TF-IDFR까지 sparse했던 단어 표현이 w2v 부터 dense해진다.
        - Contextualised Word Embedding은 단어마다 벡터가 고정되어 있지 않고 문장마다 단어의 Vector가 달라지는 Embedding 방법을 뜻한다 대표적으로 ELMo, GPT, BERT가 있다.
    - 멘토링에서 배운것
        - seq2seq 기반 한국어 pretrained STT 모델이 있긴해.
        - 발음은 모음을 토대로 만들어지는데 한국어의 모음과 영어의 모음이 거의 유사해서 영어 pretrained 모델에 한국어 finetunning을 해도 쓸만함.
        - 프로세스를 보고 내가 맡은 모듈이 정확히 어디에 들어가서 굴러가는지를 잘 알아야 하는게 신입한테 중요해.
        - 자소서는 두괄식으로


- 회고
    - 몇일을 빼먹은거야...
    - 데일리 회고 이제 다시 시작...!
    - 변성윤 마스터님 타코 비유가 너무 많아서 실제적인 개념이 더 궁금해짐.
    - 하루치 회고가 오늘따라 많은걸 보니 오늘 잘했다.
    