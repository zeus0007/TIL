# Sequence to Sequence Learning with Neural Networks 논문 요약
[Introduction, Conclusiond 단락별 요약 내용 정리]

제목을 보고 느낀점
- Sequence 와 Sequence를 연결해서 학습하나보다.
    - encoder decoder와 같은 느낌인가?
- Neural Networks를 사용한다.

메인 figure보기

![picture 2](images/889ab3f4aa3e6530c5f3265911821979668bbf5ca04c58e35a0659cea60b9a17.png)  

- ABC라는 하나의 sequence가 들어가고 \<EOS>가 들어가면 그때부터 WXYZ라는 sequence를 만들기 시작함.
- Figure 1 에보면 LSTM이 입력 문장을 반대로 읽어서 데이터들에 많은 단기 종속성이 도입되어 optimization 문제가 이 더 쉬워진다고 한다.

풀고자 하는 문제가 무엇인지 (Task 정의)

- 길이가 변할수 있는 긴 문장에 대한 번역

정리

- DNN은 강력하다.
- DNN은 dimension이 고정된 문제만 풀 수 있다. 연속적인 긴문장 번역이 어려움. 그래서 도메인과 독립적인 sequence와 sequence를 매핑하는 방식을 사용한다.
- lstm을 이용해서 sequence to sequence 문제를 해결한다.
- **한 번에 하나의 time step의 input sequence를 읽기 위해 하나의 LSTM을 사용하여 큰 고정 치수 벡터를 얻은 다음 다른 LSTM을 사용하여 해당 벡터(그림 1)에서 출력 시퀀스를 추출하는 것이다**.
- 두 번째 LSTM은 입력 시퀀스를 조건으로 한다는 점을 제외하고 본질적으로 반복 신경망 언어 모델이다.
- 이렇게 하면 긴 데이터를 성공적으로 학습할 수 있다.
- LSTM이 입력 문장을 반대로 읽어서 데이터들에 많은 단기 종속성이 도입되어 optimization 문제가 이 더 쉬워진다고 한다.
- 5개의 심층 lstm으로 번역한 결과 34.81의 bleu score 달성
- phrase-based SMT 보다 성능 향상 (Statistical Machine Translation)
- lstm은 긴문장에서 어려움을 겪지 않았다.
- 단어순서를 거꾸로 했기 때문에 긴 문장의 번역을 잘 할 수 있었다고 본다. word reversing 기법은 이 연구의 주요 기술임.
- 많은 short term dependency들을 소개
- 결과적으로 Sgd로 긴문장이 들어간 lstm을 학습할 수 있었음.

- vocab이 제한된 large deep LSTM이 어휘가 무제한인 standard SMT-based 시스템보다 강력하다.
- 긴문장을 정확히 번역할 수 있었음

한줄 요약 : encoder decoder

Sequence